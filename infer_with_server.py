import asyncio
import aiohttp
import json
import subprocess
import time
import os
import sys
import requests
from tqdm.asyncio import tqdm

# === é…ç½®éƒ¨åˆ† ===
# æ¨¡å‹ä¸ç¡¬ä»¶é…ç½®
MODEL_PATH = "/data/gaozhitao/PSP/models/psp_round_3"
SERVED_MODEL_NAME = "phybench-model"
GPU_DEVICES = "2,3"            # å¯¹åº” CUDA_VISIBLE_DEVICES
TENSOR_PARALLEL_SIZE = 2       # å¦‚æœç”¨åŒå¡è·‘ä¸€ä¸ªæ¨¡å‹è®¾ä¸º2
PORT = 8002

# æ•°æ®ä¸è¾“å‡º
INPUT_FILE = "/root/codespace/gaozhitao/PSP_bmk/phybench/dataset/PHYBench-questions_v1.json"
OUTPUT_FILE = "phybench_results_qwen2_5_7B_10_1000_ciritic_1126_round_3.jsonl"
CONCURRENCY_LIMIT = 50         

# API åœ°å€ (æœ¬åœ°)
BASE_URL = f"http://localhost:{PORT}"
API_URL = f"{BASE_URL}/v1/chat/completions"

# === 1. æœåŠ¡å™¨ç®¡ç†æ¨¡å— ===

def start_vllm_server():
    """å¯åŠ¨ vLLM API Server å­è¿›ç¨‹"""
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ vLLM æœåŠ¡å™¨ (Port: {PORT}, GPUs: {GPU_DEVICES})...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = GPU_DEVICES
    
    # æ„å»ºå¯åŠ¨å‘½ä»¤
    cmd = [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", MODEL_PATH,
        "--served-model-name", SERVED_MODEL_NAME,
        "--port", str(PORT),
        "--tensor-parallel-size", str(TENSOR_PARALLEL_SIZE),
        "--trust-remote-code",
        "--max-model-len", "4096",
        "--gpu-memory-utilization", "0.9"
    ]
    
    # ä½¿ç”¨ Popen å¯åŠ¨åå°è¿›ç¨‹
    process = subprocess.Popen(
        cmd,
        env=env,
        stdout=subprocess.DEVNULL, 
        stderr=subprocess.PIPE
    )
    return process

def wait_for_server(process, timeout=600): # å¢åŠ ä¸€ç‚¹è¶…æ—¶æ—¶é—´ä»¥é˜²æ¨¡å‹åŠ è½½æ…¢
    """è½®è¯¢ç›´åˆ°æœåŠ¡å™¨å‡†å¤‡å°±ç»ª"""
    start_time = time.time()
    health_url = f"{BASE_URL}/health"
    print("â³ ç­‰å¾…æœåŠ¡å™¨å°±ç»ª...")
    
    while True:
        # æ£€æŸ¥å­è¿›ç¨‹æ˜¯å¦æ„å¤–é€€å‡º
        if process.poll() is not None:
            stdout, stderr = process.communicate()
            print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ï¼é€€å‡ºä»£ç : {process.returncode}")
            if stderr:
                print(f"é”™è¯¯æ—¥å¿—:\n{stderr.decode()}")
            raise RuntimeError("vLLM server failed to start.")

        # å°è¯•è¿æ¥å¥åº·æ£€æŸ¥æ¥å£
        try:
            resp = requests.get(health_url, timeout=1)
            if resp.status_code == 200:
                print("âœ… æœåŠ¡å™¨å·²å°±ç»ªï¼Œå¼€å§‹æ¨ç†ï¼")
                return
        except requests.exceptions.RequestException:
            pass # è¿æ¥å¤±è´¥ï¼Œç»§ç»­ç­‰å¾…

        if time.time() - start_time > timeout:
            process.terminate()
            raise TimeoutError("ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶ã€‚")
        
        time.sleep(5)


def build_prompt(content):
    return (
        f"Question: {content}\n\n"
        "Please solve the physics problem above step-by-step. "
        "At the very end, output the final symbolic expression in LaTeX format inside a boxed command, "
        "like \\boxed{expression}. Do not include the derivation inside the box."
    )

async def fetch_response(session, item, semaphore):
    async with semaphore:
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ JSON å¯¹è±¡ä¸­çš„é”®åä¸º 'content' å’Œ 'answer'
        # å¦‚æœæŠ¥é”™ KeyErrorï¼Œè¯·æ£€æŸ¥ JSON æ–‡ä»¶ä¸­çš„é”®åæ˜¯å¦ä¸º 'Question', 'question' ç­‰